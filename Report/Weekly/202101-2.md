# 周报

by Vieran

## 本周工作总结

### QuEST

1. **无效**试试1：对热点函数statevec_controlledCompactUnitaryKernel()的调用关系和逻辑进行了分析，发现一半的线程没有进行实际的运算（只有在controlBit位的值为1的时候才进行实际的运算，但是这只有1/2的概率）

   * 修改了kernel函数的线程的分配，发现即使不分配那一部分的线程也不能加速程序，甚至还变慢了大约0.2s
   * 分析原因：做了多种实验，得到的结果如下

   | 条件                                              | qft算例 |
   | ------------------------------------------------- | ------- |
   | 暴力砍掉一半线程                                  | 7.9     |
   | 1+去掉control位（全部计算都进行，计算量加倍）     | 10.9    |
   | 1+使用最低为作为control位（隔个访问，计算量减半） | 12.9    |
   | 2+index左移+1（保持访存量不变，计算量不变）       | 25.3    |

   根据这个数据，分析得到（猜测）是由于砍掉线程之后，由于不连续的内存访问，导致性能不升反降

   

2. **无效**试试2：修改block和thread配比（vol08，cuda/10.1.243-gcc-4.8.5）

   | blocks     | threads | time：random/qft    |
   | ---------- | ------- | ------------------- |
   | 2**29/128  | 128     | /13.094768          |
   | 2**29/8    | 8       | /39.259119          |
   | 2**29/256  | 256     | /13.067425          |
   | 2**29/512  | 512     | /13.057939          |
   | 2**29/1024 | 1024    | 17.099122/13.028273 |
   | 2**29/2048 | 2048    | /2.657656   出错    |

   从这一份数据也可以看出来，修改block和thread并不能改善其性能，理由同样是由于内存访问没有改善，所以并没有得到加速

   *但是这里为什么线程超过一定数量之后，会直接出错，我并不太理解*
   
   

**代码阅读**

1. kernel函数都非常耗时，不仅仅是statevec_compactUnitaryKernel，其他的kernel函数也一样（对比qft和random的profile），在qft中热点集中只是因为qft调用的某一个函数从次数比较多，其实还有其他的kernel函数更耗时的（比如在random的profile可以看出）
2. 完成对jsl学长的cpu代码的阅读（终于读完了），对于其优化的思想有一定了解（是我想不出来的优化）



### 基础知识

> 1. 阅读roofline论文，对理论性能分析作了初步了解
> 2. 学习CUDA的内存模型相关的内容
> 3. 了解CPU流水线知识、局部性原理



## 下周任务

- [ ] 尝试进行访存优化（不知道能不能做出来，反正先试试
- [ ] 继续学习hpc书的内容