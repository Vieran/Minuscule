# 程序运行时间记录

```bash
#将profile的标准输出和标准错误输出都重定向到文件xxx
nvprof ./demo &>xxx
#2>&1 意思是把“标准错误输出”重定向到“标准输出”

#baseline：target=29，halfblock=4G，block=8G，total=2*8G=16G
```



## 多节点数据(opt)

```bash
#3节点*40核心，random算例（16G），优化版本
#baseline：58.46s
mpirun -np 128 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #47.42s；这个或许是无效的，因为核心没那么多
mpirun -np 64 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #44.47s
mpirun -np 32 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #41.56s
mpirun -np 16 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #41.09s
mpirun -np 8 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #40.65s
mpirun -np 4 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #49.30s
mpirun -np 2 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #63.42s
mpirun -np 1 -machinefile mf -genv I_MPI_DEBUG=5 ./demo #59.81s

#8节点*40核心，交换包大小取决于进程数目，交换次数取决于门和target（若交换，必为n/2次）
#由于是random算例，无法统计交换次数（或者说，统计这个很麻烦，得写个脚本）；若需要统计，得造一个更加规则的算例再测
mpirun -machinefile mf -genv I_MPI_DEBUG=5 -n 1 ./demo
#1，无交换，59.67s
#2，交换大小为8G，67.48s
#4，交换大小为4G，45.78s
#8，交换大小为1G，28.23s
#16，交换大小为512M，19.61s
#32，交换大小为256M，18.85s
#64，交换大小为128M，16.37s
#128，交换大小为64M，15.72s
#256，交换大小为32M，15.29s
#512，交换大小为16M，16.60s

#4节点*40核心，qubits=34(2*256G)，rotateXYZ门*15个，target=31(2组*16G*2pair)
#4[1,1,1,1]，64G>=32G，无通信，5.14s
#8[2,2,2,2]，32G>=32G，无通信，248.23s；16G<32G，16G*4对
#8[1,2,3,2]，32G>=32G，无通信，1430.80s
#16[4,4,4,4]，16G<32G，16G*8对*2次/对*15个门，151.77s
#32[8,8,8,8]，8G<32G，8G*16对*2次/对*15个门，85.88s
#64[16,16,16,16]，4G<32G，4G*32对*2次/对*15个门，57.29s
#128[32,32,32,32]，2G<32G，2G*64对*2次/对*15个门，47.67s
#256[64,64,64,64]，1G<32G，1G*128对*2次/对*15个门，51.38s
```

**一些总结**

1. machine file分配的进程可能是不均匀的，需要自己查看具体到底是怎么分配的

2. 从3*40那组结果看，似乎看不出来什么

3. ~~从8*40那组结果看，跑满全部节点的情况下，尽可能使用更多的进程会减少时间（在进程总数<cpu核心数的前提下）~~

4. 优化版本：跑满全部节点的情况下，尽可能让进程数少，比如每个节点只跑一个进程。当单个节点的进程数量大于1个时，时间先增大后减小

5. ~~优化版本的多进程表现和基础版本完全不一样？？？（均通过了正确性检验~~

   本质上是一样的，不需要MPI通信是最佳选择

6. 进程分配不均匀对结果影响非常大



## 多节点数据（random）

| 进程数量 | 进程分配      | baseline                     | opt    |
| -------- | ------------- | ---------------------------- | ------ |
| 1        | [1,0,0,0]     | 97.77s                       | 60.39s |
| 2        | [1,1,0,0]     | 99.51s                       | 61.88s |
| 4        | [1,1,1,1]     | 96.49s                       | 39.15s |
| 8        | [2,2,2,2]     | 184.55s                      | 35.04s |
| 16       | [4,4,4,4]     | 358.48s                      | 32.38s |
| 32       | [8,8,8,8]     | 701.95s                      | 29.01s |
| 64       | [16,16,16,16] | 1869.55s<br>~3157.67s~killed | 27.46s |
| 128      | [32,32,32,32] | ---                          | 26.27s |
| 256      | [64,64,64,64] | ---                          | 30.91s |

结论：

1. 算例不规整导致变化十分不规则
2. 从这里看出，通信次数的增大会导致时间的变化发生巨大的变化



**qubits=34，target=31(2\*16G，halfblock，32G block)，4*40核心，rotate门\*15，总内存2\*128G\*2**

| 进程数    | 进程分配          | 通信                     | baseline | opt     |
| --------- | ----------------- | ------------------------ | -------- | ------- |
| 4, 32G    | [1,1,1,1]         | 无                       | 38.97s   | 5.14s   |
| 8, 16G    | [1,1,1,1,1,1,1,1] | 16G\*4对\*2数组\*15门\*2 | 114.78s  | 108.66s |
| 8, 16G    | [2,2,2,2]         | 16G\*4对\*2数组\*15门\*2 | 234.48s  | 248.23s |
| 16, 8G    | [4,4,4,4]         | 8G\*8\*2\*15             | 151.68s  | 151.77s |
| 32, 4G    | [8,8,8,8]         | 4G\*16\*2\*15            | 94.05s   | 85.88s  |
| 64, 2G    | [16,16,16,16]     | 2G\*32\*2\*15            | 71.20s   | 57.29s  |
| 128, 1G   | [32,32,32,32]     | 1G\*64\*2\*15            | 62.49s   | 47.67s  |
| 256, 512M | [64,64,64,64]     | 512M\*128\*2\*15         | 68.30s   | 51.38s  |

结论：

1. MPI通信，总的数据量不变，随着单次通信的数据量增大，符合线性关系——通信延时（传输延时，传播延时，处理延时，排队延时）
2. MPI的contition schedule，结构体+查询
3. 即使是最少的进程也需要通信